{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd5ad2a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 5: Deep Learning Computation, Convolutional Neural Networks-1\n",
    "\n",
    " **Reading**:   Chapter 5.1-5.2, 6.1-6.3 of *Dive Into Deep Learning*\n",
    "\n",
    "## Outline\n",
    "\n",
    "- Deep Learning Computation: Model Construction and Parameter Management\n",
    "- Convolutional Neural Networks: Convolution, Padding and Stride\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9771a67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Construction: Layers and Blocks\n",
    "\n",
    "\n",
    "- **A single neuron**\n",
    "  - takes some set of inputs; \n",
    "  - generates a corresponding scalar output with a linear model and a activation function; and \n",
    "  - has a set of associated parameters that can be learned to optimize some objective function of interest. \n",
    "- **A layer**\n",
    "  - Multiple neurons taking the same set of inputs;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de287727",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **MLP**\n",
    "  - Multiple layers stacked together whethe the inputs of a layer is the outputs of its preceding layer\n",
    "- **A block**\n",
    "  -  larger than an individual layer but smaller than the entire model.\n",
    "  - Can be a single layer, a component consisting of multiple layers, or the entire model itself.\n",
    "  - Multiple layers are combined into blocks, forming repeating patterns of larger models.\n",
    "  - Benefit: compact code to implement complex neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa8bbed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Multiple layers are combined into blocks, forming repeating patterns of larger models.](../img/blocks.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12da7d84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "  - From a programing standpoint, a block is represented by a class. \n",
    "    - Any subclass of it must define a forward propagation function that transforms its input into output and must store any necessary parameters.\n",
    "    - Finally a block must possess a backpropagation function, for purposes of calculating gradients. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3274143",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following code generates a network with one fully-connected hidden layer with 256 units and ReLU activation, followed by a fully-connected output layer with 10 units (no activation function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ef6bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0864, -0.0297,  0.0581,  0.0375, -0.1816,  0.1452, -0.0222, -0.1160,\n",
       "         -0.1659, -0.2437],\n",
       "        [ 0.1028, -0.0994,  0.1481,  0.1704, -0.1635,  0.1939, -0.0030,  0.0426,\n",
       "         -0.1631, -0.2410]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5c6a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic functionality of a block\n",
    "1. Ingest input data as arguments to its forward propagation function.\n",
    "1. Generate an output by having the forward propagation function return a value.\n",
    "1. Calculate the gradient of its output with respect to its input, which can be accessed via its backpropagation function. \n",
    "1. Store and provide access to those parameters necessary\n",
    "   to execute the forward propagation computation.\n",
    "1. Initialize model parameters as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8967ad",
   "metadata": {
    "origin_pos": 43,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary for Model Construction\n",
    "\n",
    "* Layers are blocks.\n",
    "* Many layers can comprise a block.\n",
    "* Many blocks can comprise a block.\n",
    "* A block can contain code.\n",
    "* Blocks take care of lots of housekeeping, including parameter initialization and backpropagation.\n",
    "* Sequential concatenations of layers and blocks are handled by the `Sequential` block.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8fe905",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parameter Management\n",
    "\n",
    "\n",
    "* Accessing parameters for debugging, diagnostics, and visualizations.\n",
    "* Parameter initialization.\n",
    "* Sharing parameters across different model components.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8124820",
   "metadata": {
    "origin_pos": 2,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0048],\n",
       "        [0.0381]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee653b6f",
   "metadata": {
    "origin_pos": 4,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameter Access\n",
    "\n",
    "- Access any layer by indexing into the model as though it were a list.\n",
    "- Inspect the parameters of the second fully-connected layer as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f4108f4",
   "metadata": {
    "origin_pos": 6,
    "scrolled": false,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[0.0865, 0.0056, 0.2186, 0.3380, 0.0305, 0.2811, 0.0094, 0.3424]])), ('bias', tensor([-0.1197]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe5d43",
   "metadata": {
    "origin_pos": 8,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "### Targeted Parameters\n",
    "\n",
    "\n",
    "- Extracts the bias from the second neural network layer and further accesses that parameter's value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bdbd92a",
   "metadata": {
    "origin_pos": 10,
    "scrolled": true,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.1197], requires_grad=True)\n",
      "tensor([-0.1197])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0143d88",
   "metadata": {
    "origin_pos": 12,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "-  Access the gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c37faf29",
   "metadata": {
    "origin_pos": 14,
    "scrolled": true,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c5bd6",
   "metadata": {
    "origin_pos": 15,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### All Parameters at Once\n",
    "\n",
    "- Accessing the parameters of the first fully-connected layer vs. accessing all layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eff54443",
   "metadata": {
    "origin_pos": 17,
    "scrolled": true,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fddf9bd",
   "metadata": {
    "origin_pos": 19,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Accessing the parameters of the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ef3f92",
   "metadata": {
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0865, 0.0056, 0.2186, 0.3380, 0.0305, 0.2811, 0.0094, 0.3424]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.weight'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecd912f",
   "metadata": {
    "origin_pos": 23,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Collecting Parameters from Nested Blocks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abe60071",
   "metadata": {
    "origin_pos": 25,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0460],\n",
       "        [0.0460]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # Nested here\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aba7547b",
   "metadata": {
    "origin_pos": 29,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc217086",
   "metadata": {
    "origin_pos": 31,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Access the first major block, \n",
    "- within it the second sub-block, and \n",
    "- within that the bias of the first layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3760da08",
   "metadata": {
    "origin_pos": 33,
    "scrolled": true,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1604, -0.4100, -0.1052, -0.0569, -0.1441, -0.3547, -0.4200,  0.2776])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769328d5",
   "metadata": {
    "origin_pos": 35,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameter Initialization\n",
    "\n",
    "- PyTorch's `nn.init` module provides a variety of preset initialization methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81abf0c",
   "metadata": {
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ]
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4886516",
   "metadata": {
    "origin_pos": 39,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Built-in Initialization\n",
    "\n",
    "- initializes all weight parameters as Gaussian random variables\n",
    "- with standard deviation 0.01,\n",
    "- while bias parameters cleared to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1884c204",
   "metadata": {
    "origin_pos": 41,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0036,  0.0096, -0.0100, -0.0133]), tensor(0.))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697bb34b",
   "metadata": {
    "origin_pos": 43,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- initialize all the parameters to a given constant value (say, 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9adbfe40",
   "metadata": {
    "origin_pos": 45,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d570d",
   "metadata": {
    "origin_pos": 47,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Apply different initializers for certain blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c2f45d7",
   "metadata": {
    "origin_pos": 49,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0153, -0.4413, -0.2382,  0.4730])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139f7879",
   "metadata": {
    "origin_pos": 63,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tied Parameters (Shared weights)\n",
    "\n",
    "- Share weights across multiple layers.\n",
    "- The gradients of the shared layers are added together during backpropagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a32c7de",
   "metadata": {
    "origin_pos": 65,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# We need to give the shared layer a name so that we can refer to its\n",
    "# parameters\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "net(X)\n",
    "# Check whether the parameters are the same\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# Make sure that they are actually the same object rather than just having the\n",
    "# same value\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed82a6f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "\n",
    "- Some data (e.g. images) has special structures (e.g. two-dimensional grid of pixels).\n",
    "- MLP discarded each image's spatial structure by flattening them into one-dimensional vectors\n",
    "- MLPs networks are invariant to the order of the features.\n",
    "- Convolutional neural networks (CNNs), a powerful family of neural networks to overcome this limitation.\n",
    "- We will walk through the basic operations of CNNs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad701d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From Fully-Connected Layers to Convolutions\n",
    "\n",
    "\n",
    "- Tabular data: consist of rows corresponding to examples and columns corresponding to features.\n",
    "  - Patterns: interactions among the features,\n",
    "  - No assumption on the structure on how the features interact.\n",
    "  - Lack  knowledge to guide the construction of craftier architectures.\n",
    "  - An MLP may be the best that we can do.\n",
    "\n",
    "- CNN exploits some of the known structure in natural images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a3956",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Invariance\n",
    "\n",
    "- Image classification \n",
    "  - Varying location of the object in the image.\n",
    "  - Object is just part of the image. \n",
    "  \n",
    "-  Guidelines for the design of a neural network architecture suitable for computer vision:\n",
    "\n",
    "   - *translation invariance* \n",
    "   - *locality* principle: focus on local regions\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb41ee1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Convolutions\n",
    "\n",
    "- Constraining MLP\n",
    "- Same linear transformation applied on every patch (of the image)  with the same size of the convolution kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e958d4",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutions for Images\n",
    "\n",
    "\n",
    "## The Cross-Correlation Operation\n",
    "\n",
    "\n",
    "\n",
    "![Two-dimensional cross-correlation operation. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\times0+1\\times1+3\\times2+4\\times3=19$.](../img/correlation.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00470491",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Slide the convolution window across the input tensor, both from left to right and top to bottom.\n",
    "- The input subtensor contained in that window and the kernel tensor are multiplied elementwise\n",
    "- The resulting tensor is summed up yielding a single scalar value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94c2ca",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Output size\n",
    "\n",
    "- Input size $n_h \\times n_w$ \n",
    "- Convolution kernel $k_h \\times k_w$\n",
    "- The outsize is then\n",
    "via\n",
    "\n",
    "$$(n_h-k_h+1) \\times (n_w-k_w+1).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01bb6c78",
   "metadata": {
    "origin_pos": 2,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0707fa7",
   "metadata": {
    "origin_pos": 3,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def corr2d(X, K):  #@save\n",
    "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "484abfe1",
   "metadata": {
    "origin_pos": 6,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23236b1",
   "metadata": {
    "origin_pos": 7,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Layers\n",
    "\n",
    "- A convolutional layer cross-correlates the input and kernel and adds a scalar bias to produce an output.\n",
    "- Parameters:  the kernel and the scalar bias.\n",
    "- Notation: $h \\times w$ convolutional layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7449a47",
   "metadata": {
    "origin_pos": 9,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ecbb92",
   "metadata": {
    "origin_pos": 11,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Object Edge Detection in Images\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8946cb34",
   "metadata": {
    "origin_pos": 12,
    "scrolled": true,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X\n",
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc3801",
   "metadata": {
    "origin_pos": 14,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Construct a kernel `K` ($[1,-1]$ with a height of 1 and a width of 2.\n",
    "- When we perform the cross-correlation operation with the input,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f7c5b98",
   "metadata": {
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "K = torch.tensor([[1.0, -1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "717f36b8",
   "metadata": {
    "origin_pos": 17,
    "scrolled": true,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33945929",
   "metadata": {
    "origin_pos": 18,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " - Apply the kernel to the transposed image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "100757de",
   "metadata": {
    "origin_pos": 19,
    "scrolled": true,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.t(), K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc151e00",
   "metadata": {
    "origin_pos": 20,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Kernels\n",
    "\n",
    "- The kernels in CNNs are learnt similarly as the weights of MLPS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5aeeb",
   "metadata": {
    "origin_pos": 28,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Correlation and Convolution\n",
    "\n",
    "\n",
    "- Convolution operations are usually implemented with cross-correlation\n",
    "- For strict *convolution* operation, \n",
    "  - flip the two-dimensional kernel tensor both horizontally and vertically, and\n",
    "  - perform the *cross-correlation* operation with the input tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f7b7e",
   "metadata": {
    "origin_pos": 28,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Map and Receptive Field\n",
    "\n",
    "- *feature map*: the output of convolutional layer\n",
    "   - learned representations (features)\n",
    "   - *receptive field* of any element $x$of some layer: all the elements (from all the previous layers) that may affect the calculation of $x$ during the forward propagation.\n",
    "   - The *receptive field* of the elements in the late layers is getting larger than than those of the preceding layers.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3940c78",
   "metadata": {
    "origin_pos": 28,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Summary of Convolution\n",
    "\n",
    "* The core computation of a two-dimensional convolutional layer is a two-dimensional cross-correlation operation.\n",
    "* We can design a kernel to detect edges in images.\n",
    "* We can learn the kernel's parameters from data.\n",
    "* When any element in a feature map needs a larger receptive field to detect broader features on the input, a deeper network can be considered.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e231f61c",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Padding and Stride\n",
    "\n",
    "\n",
    "## Padding\n",
    "\n",
    "-  Pad a $3 \\times 3$ input with zeros increasing its size to $5 \\times 5$.\n",
    "- The corresponding output then increases to a $4 \\times 4$ matrix.\n",
    "\n",
    "![Two-dimensional cross-correlation with padding.](../img/conv-pad.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7257716f",
   "metadata": {
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    "- Add a total of $p_h$ rows of padding (roughly half on top and half on bottom) and \n",
    "- a total of $p_w$ columns of padding (roughly half on the left and half on the right),\n",
    "- the output shape will be\n",
    "\n",
    "$$(n_h-k_h+p_h+1)\\times(n_w-k_w+p_w+1).$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fec8333d",
   "metadata": {
    "origin_pos": 2,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# We define a convenience function to calculate the convolutional layer. This\n",
    "# function initializes the convolutional layer weights and performs\n",
    "# corresponding dimensionality elevations and reductions on the input and\n",
    "# output\n",
    "def comp_conv2d(conv2d, X):\n",
    "    # Here (1, 1) indicates that the batch size and the number of channels\n",
    "    # are both 1\n",
    "    X = X.reshape((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    # Exclude the first two dimensions that do not interest us: examples and\n",
    "    # channels\n",
    "    return Y.reshape(Y.shape[2:])\n",
    "# Note that here 1 row or column is padded on either side, so a total of 2\n",
    "# rows or columns are added\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)\n",
    "X = torch.rand(size=(8, 8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40cf5268",
   "metadata": {
    "origin_pos": 6,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we use a convolution kernel with a height of 5 and a width of 3. The\n",
    "# padding numbers on either side of the height and width are 2 and 1,\n",
    "# respectively\n",
    "conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34c4dc",
   "metadata": {
    "origin_pos": 8,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stride\n",
    "\n",
    "\n",
    "\n",
    "- *Stride*: the number of rows and columns traversed per slide\n",
    "\n",
    "![Cross-correlation with strides of 3 and 2 for height and width, respectively.](../img/conv-stride.svg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88d83643",
   "metadata": {
    "origin_pos": 10,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aece15",
   "metadata": {
    "origin_pos": 12,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, we will look at (**a slightly more complicated example**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71874930",
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea07ae0",
   "metadata": {
    "origin_pos": 16,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Summary of Padding and Stride\n",
    "\n",
    "* Padding can increase the height and width of the output. This is often used to give the output the same height and width as the input.\n",
    "* The stride can reduce the resolution of the output, for example reducing the height and width of the output to only $1/n$ of the height and width of the input ($n$ is an integer greater than $1$).\n",
    "* Padding and stride can be used to adjust the dimensionality of the data effectively.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240633f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
